{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "import numpy as np\n",
    "import pysnooper\n",
    "from pandas import DataFrame\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from collections import defaultdict\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "class KeysManager:\n",
    "    def __init__(self):\n",
    "        self.keys,self.temp = self.get_keys()\n",
    "        self.wait_craw_key = self.keys - self.temp\n",
    "    def get_keys(self):\n",
    "        with open('./拓展关键词组合.txt',encoding='utf-8') as f:\n",
    "            keys = set(f.read().split())\n",
    "        if os.path.exists('./temp.txt'):\n",
    "            with open('./temp.txt',encoding='utf-8') as f:\n",
    "                temp = set(f.read().split())\n",
    "        else:\n",
    "            temp = set()\n",
    "        return keys,temp\n",
    "    def save_key(self,key):\n",
    "        with open('./temp.txt','a+',encoding='utf-8') as f:\n",
    "            f.write(key+' ')\n",
    "    def get_key(self):\n",
    "        key = self.wait_craw_key.pop().strip()\n",
    "        return key\n",
    "\n",
    "\n",
    "class Crawler:\n",
    "    def __init__(self):\n",
    "        self.cookies = None\n",
    "        self.sign_in_web = \"https://www.lagou.com/landing-page/pc/search.html?utm_source=m_cf_cpt_baidu_pcbt\"\n",
    "        self.driver_config()\n",
    "        self.key_manager = KeysManager()\n",
    "        self.save_path='./work_data'\n",
    "    def driver_config(self):\n",
    "        '''\n",
    "        用于设置反反爬配置\n",
    "        :return:\n",
    "        '''\n",
    "        chrome_options = Options()\n",
    "        # chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument('--window-size=1920x1080')\n",
    "        chrome_options.add_argument(\n",
    "            'user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36')\n",
    "        self.webdriver = Chrome(options=chrome_options)\n",
    "        with open('./stealth.min.js') as f:\n",
    "            js = f.read()\n",
    "\n",
    "        self.webdriver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "            \"source\": js\n",
    "        })\n",
    "\n",
    "\n",
    "    def sign_in(self):\n",
    "        '''\n",
    "        登录，并保存cookies\n",
    "        :return:\n",
    "        '''\n",
    "        self.webdriver.get(self.sign_in_web)\n",
    "        self.webdriver.find_element_by_class_name(\"login-by-account\").click()\n",
    "        time.sleep(1)\n",
    "        windows = self.webdriver.window_handles\n",
    "        self.webdriver.close()\n",
    "        self.webdriver.switch_to.window(windows[1])\n",
    "        self.webdriver.find_element_by_xpath(\"//input[@placeholder='请输入常用手机号/邮箱']\").send_keys('账号')\n",
    "        self.webdriver.find_element_by_xpath(\"//input[@placeholder='请输入密码']\").send_keys('密码')\n",
    "        self.webdriver.find_element_by_class_name(\"no-checked\").click()\n",
    "        self.webdriver.find_element_by_xpath(\"//input[@type='submit']\").click()\n",
    "        self.save_cookies()\n",
    "    def save_cookies(self):\n",
    "        '''\n",
    "        存储cookies\n",
    "        :param driver:\n",
    "        :return:\n",
    "        '''\n",
    "        tb_cookies = self.webdriver.get_cookies()\n",
    "        cookies = {}\n",
    "        for item in tb_cookies:\n",
    "            cookies[item['name']] = item['value']\n",
    "        output_path = open('.\\cookies.pickle', 'wb')\n",
    "        pickle.dump(cookies, output_path)\n",
    "        output_path.close()\n",
    "\n",
    "    def load_cookies(self):\n",
    "        '''\n",
    "        加载cookies\n",
    "        :return:\n",
    "        '''\n",
    "        self.webdriver.get(\"https://www.lagou.com\")\n",
    "        if os.path.exists('cookies.pickle'):\n",
    "            read_path = open('cookies.pickle', 'rb')\n",
    "            tb_cookies = pickle.load(read_path)\n",
    "            read_path.close()\n",
    "        for cookie in tb_cookies:\n",
    "            self.webdriver.add_cookie({\n",
    "                \"domain\": '.lagou.com',\n",
    "                \"name\": cookie,\n",
    "                \"value\": tb_cookies[cookie],\n",
    "            })\n",
    "\n",
    "    @pysnooper.snoop(prefix=\"get_jobs: \")\n",
    "    def get_jobs(self):\n",
    "        '''\n",
    "        获取搜索页中所有工作\n",
    "        :return:\n",
    "        '''\n",
    "        #windows = self.webdriver.window_handles\n",
    "        #self.webdriver.switch_to.window(windows[1])\n",
    "        jobs_abstract = []\n",
    "        time.sleep(5)\n",
    "        position_list = self.webdriver.find_elements_by_class_name(\"list_item_top\")\n",
    "        for i in range(len(position_list)):\n",
    "            job_info = self.webdriver.find_elements_by_class_name(\"list_item_top\")[i]\n",
    "            position = job_info.find_element_by_class_name(\"position\")\n",
    "            position = position.text.split(\"\\n\")\n",
    "            name = position[0].strip()\n",
    "            if len(name)>10:\n",
    "                name = name[:10]\n",
    "            location = re.search(r'[^\\W]+', position[1]).group(0)\n",
    "            other_info = position[3].split(\" \")\n",
    "            salary = other_info[0].strip()\n",
    "            experience = other_info[1].strip()\n",
    "            edu_bg = other_info[3].strip()\n",
    "\n",
    "            try:\n",
    "                company_ele = job_info.find_element_by_class_name(\"company\")\n",
    "            \n",
    "            \n",
    "                company = company_ele.text.split(\"\\n\")\n",
    "                company_name = company[0].strip()\n",
    "                other_info = company[1].split(\" \")\n",
    "                industry = other_info[0].strip()\n",
    "                company_scale = other_info[4].strip()\n",
    "                jobs_abstract.append([name, salary, experience, edu_bg, location,company_name, industry, company_scale])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(company_ele.get_attribute(\"outerHTML\"))\n",
    "            \n",
    "        jobs_abstract = DataFrame(np.array(jobs_abstract),columns=[\"name\", \"salary\", \"experience\", \"edu_bg\", \"location\",\"company_name\", \"industry\", \"company_scale\"])\n",
    "        return  jobs_abstract\n",
    "    def search(self,key):\n",
    "        '''\n",
    "        根据关键字搜索相关工作\n",
    "        :param key:\n",
    "        :return:\n",
    "        '''\n",
    "        web_path = \"https://www.lagou.com/jobs/list_\" + key\n",
    "        #self.load_cookies()\n",
    "        self.webdriver.get(web_path)\n",
    "        time.sleep(3)\n",
    "        self.save_cookies()\n",
    "    #@pysnooper.snoop(prefix=\"get_job_describtion: \")\n",
    "    def get_job_describtion(self):\n",
    "        '''\n",
    "        获取每个具体工作的职责和要求\n",
    "        :return:\n",
    "        '''\n",
    "        jobs_describtion = []\n",
    "        position_page_list = self.webdriver.find_elements_by_class_name(\"position_link\")\n",
    "        for i in range(len(position_page_list)):\n",
    "            try:\n",
    "                \n",
    "                position_page = self.webdriver.find_elements_by_xpath(\"//a[@class='position_link']\")[i]\n",
    "                element = position_page.find_element_by_tag_name(\"h3\")\n",
    "                #position_page.send_keys(Keys.DOWN)\n",
    "                #\n",
    "                \n",
    "                #self.webdriver.execute_script(\"arguments[0].scrollIntoView();\", element)\n",
    "                #js=\"var q=document.documentElement.scrollTop=10000\"\n",
    "                #self.webdriver.execute_script(js)\n",
    "                \n",
    "                action = ActionChains(self.webdriver).move_to_element_with_offset(element,5,5)\n",
    "                action.send_keys(Keys.DOWN)\n",
    "                action.send_keys(Keys.DOWN)\n",
    "                action.send_keys(Keys.DOWN)\n",
    "                action.perform()\n",
    "                time.sleep(2)\n",
    "                position_page = self.webdriver.find_elements_by_xpath(\"//a[@class='position_link']\")[i]\n",
    "                element = position_page.find_element_by_tag_name(\"h3\")\n",
    "                #action.move_to_element_with_offset(element,0,0).click().perform()\n",
    "                element.click()\n",
    "                time.sleep(5)\n",
    "                windows = self.webdriver.window_handles\n",
    "                self.webdriver.switch_to.window(windows[1])\n",
    "                WebDriverWait(self.webdriver, 10).until(\n",
    "                    expected_conditions.presence_of_element_located((By.CLASS_NAME, \"job-detail\")))\n",
    "                position_head = self.webdriver.find_element_by_class_name(\"position-head-wrap-name\")\n",
    "                position_name = position_head.find_element_by_class_name(\"position-head-wrap-position-name\").text.strip()\n",
    "                if len(position_name) > 10:\n",
    "                    position_name = position_name[:10]\n",
    "                position_salary = position_head.find_element_by_class_name(\"salary\").text.split('·')[0].strip()\n",
    "                position_describtion = self.webdriver.find_element_by_class_name(\"job-detail\").text\n",
    "                self.webdriver.close()\n",
    "                self.webdriver.switch_to.window(windows[0])\n",
    "                position_describtion = position_describtion.split(\"\\n\")\n",
    "                responsibility = []\n",
    "                demand = []\n",
    "                check_code = 0\n",
    "\n",
    "                for index, text in enumerate(position_describtion):\n",
    "                    if check_code == 0 and re.match(r\"1|-|•\", text):\n",
    "                        check_code = 1\n",
    "                    if check_code == 1:\n",
    "                        if re.match(r\"\\d|-|•\", text):\n",
    "                            responsibility.append(text)\n",
    "                        else:\n",
    "                            check_code = 2\n",
    "                    if check_code == 2 and re.match(r\"\\d|-|•\", text):\n",
    "                        demand.append(text)\n",
    "                #如果上述筛选方式，无法拆分工作描述，则将其放到responsibility中\n",
    "                if check_code == 0:\n",
    "                    responsibility = position_describtion\n",
    "                jobs_describtion.append([position_name,position_salary,responsibility,demand])\n",
    "            except:\n",
    "                jobs_describtion.append([position_name,position_salary[\"null\"],[\"null\"]])\n",
    "        jobs_describtion = DataFrame(np.array(jobs_describtion),columns=[\"name\",\"salary\",\"responsibility\",\"demand\"])\n",
    "        return jobs_describtion\n",
    "    def get_next_page(self):\n",
    "        '''\n",
    "        进入下一页\n",
    "        :return:\n",
    "        '''\n",
    "        time.sleep(1)\n",
    "        #element = self.webdriver.find_element_by_class_name(\"pager_next\")      \n",
    "        #ActionChains(self.webdriver).move_to_element_with_offset(element,5,5).click().perform()\n",
    "#         action.send_keys(Keys.DOWN)\n",
    "#         action.send_keys(Keys.DOWN)\n",
    "#         action.send_keys(Keys.DOWN)\n",
    "#         action.perform()\n",
    "#         time.sleep(2)\n",
    "        \n",
    "        self.webdriver.find_element_by_class_name(\"pager_next\").click()\n",
    "    def save_jobs_info(self,key,jobs_abstract,jobs_describtion):\n",
    "        '''\n",
    "        将爬取到的数据存入csv中\n",
    "        :param key:\n",
    "        :param jobs_abstract:\n",
    "        :param jobs_describtion:\n",
    "        :return:\n",
    "        '''\n",
    "        jobs_info = pd.merge(jobs_abstract,jobs_describtion,on=['name','salary'])\n",
    "        jobs_info.drop_duplicates(list(jobs_info.columns)[:8],inplace=True)\n",
    "        jobs_info['key'] = key\n",
    "        file_path = os.path.join(self.save_path,'%s.csv'%key)\n",
    "        if not os.path.exists(file_path):\n",
    "            # ['name', 'salary', 'experience', 'edu_bg',\n",
    "            #         'location','company_name', 'industry', 'company_scale','responsibility','demand','key']\n",
    "            head = jobs_info.columns\n",
    "            with open(file_path, 'w+',encoding='utf-8',newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(head)\n",
    "        with open(file_path,'a+',encoding='utf-8',newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for index in range(len(jobs_info)):\n",
    "                writer.writerow(jobs_info.iloc[index,:].values)\n",
    "\n",
    "    @pysnooper.snoop(prefix=\"crawer: \")\n",
    "    def crawer(self,key,sign_in=False,job_num=50):\n",
    "        if sign_in==False:\n",
    "            self.sign_in()\n",
    "            x=input()\n",
    "        #self.load_cookies()\n",
    "        self.search(key)\n",
    "        job_count = 0\n",
    "        for i in range(30):\n",
    "            try:\n",
    "                if job_count<job_num:\n",
    "                    jobs_abstract = self.get_jobs()\n",
    "                    jobs_describtion = self.get_job_describtion()\n",
    "                    self.save_jobs_info(key,jobs_abstract,jobs_describtion)\n",
    "                    job_count += len(jobs_abstract)\n",
    "                    try:\n",
    "                        self.get_next_page()\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        break\n",
    "                    self.save_cookies()\n",
    "            except Exception as e:\n",
    "                time_now = time.strftime('[%Y-%m-%d %H:%M:%S]',time.localtime(time.time()))\n",
    "                print('info: '+str(time_now)+' '+str(key)+' '+str(e))\n",
    "                break\n",
    "        #检测csv文件中有多少个\n",
    "        with open(\"%s/%s.csv\"%(self.save_path,key),encoding='utf-8') as f:\n",
    "            csv_length = len(f.readlines())\n",
    "        #判断数据是否被保存\n",
    "        if csv_length <=1:\n",
    "            os.remove(\"%s/%s.csv\"%(self.save_path,key))\n",
    "        else:\n",
    "            self.key_manager.save_key(key)\n",
    "\n",
    "def main():\n",
    "    crawler = Crawler()\n",
    "    key = crawler.key_manager.get_key()\n",
    "    crawler.crawer(key, True, job_num=50)\n",
    "    while(crawler.key_manager.wait_craw_key):\n",
    "        key = crawler.key_manager.get_key()\n",
    "        crawler.crawer(key, True, job_num=50)\n",
    "if __name__ == '__main__':\n",
    "    #main()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Crawler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d147195dfa26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcrawler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrawler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign_in\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'数据分析'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcrawler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_elements_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"//a[@class='position_link']\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Crawler' is not defined"
     ]
    }
   ],
   "source": [
    "crawler = Crawler()\n",
    "crawler.sign_in()\n",
    "time.sleep(10)\n",
    "crawler.search('数据分析')\n",
    "a = crawler.webdriver.find_elements_by_xpath(\"//a[@class='position_link']\")\n",
    "crawler.get_next_page()\n",
    "b = crawler.webdriver.find_elements_by_xpath(\"//a[@class='position_link']\")\n",
    "print(a[0].get_attribute('href'))\n",
    "a==b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重构代码\n",
    "- 1.登录界面后，搜索关键词，把所有的positionid提取出来\n",
    "- 2.使用requests去访问所有positionid的网址，使用BeautifulSoup解析网页\n",
    "- 3.使用ip代理，识别Ip被禁的情况，并更换Ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib3.exceptions import ProxyError,MaxRetryError\n",
    "import requests\n",
    "class Crawler_for_positionid(Crawler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def get_positionid(self,key,job_num):\n",
    "        self.search(key)\n",
    "        positionid_list=[]\n",
    "        try:\n",
    "            for i in range(30):\n",
    "                target = self.webdriver.find_elements_by_xpath(\"//a[@class='position_link']\")\n",
    "                print(len(target))\n",
    "                for j in range(len(target)):\n",
    "\n",
    "                    href = self.webdriver.find_elements_by_xpath(\"//a[@class='position_link']\")[j].get_attribute(\"href\")\n",
    "                    positionid = re.search(r'\\d+',href).group(0)\n",
    "                    positionid_list.append(positionid)\n",
    "                check = self.webdriver.find_elements_by_xpath(\"//a[@class='position_link']\")[0].get_attribute(\"href\")\n",
    "                self.get_next_page()\n",
    "                time.sleep(2)\n",
    "                new_check = self.webdriver.find_elements_by_xpath(\"//a[@class='position_link']\")[0].get_attribute(\"href\")\n",
    "                if check==new_check or len(positionid_list)>job_num:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(key+\": \")\n",
    "            print(e)\n",
    "            \n",
    "        return positionid_list\n",
    "    @classmethod\n",
    "    def html_from_positionid(self,ip,positionid):\n",
    "        url = \"https://www.lagou.com/wn/jobs/\"+positionid+\".html\"\n",
    "        headers = {'User-agent':\"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.81 Safari/537.36 SE 2.X MetaSr 1.0\"}\n",
    "        http = urllib3.ProxyManager(ip,timeout=5,retries=4)\n",
    "        #http = urllib3. PoolManager()\n",
    "        html = http.request('GET',url,headers=headers).data\n",
    "        return html\n",
    "    @classmethod\n",
    "    def get_html_info(self,html):\n",
    "        bsobj = BeautifulSoup(html)\n",
    "        position_name = bsobj.find('span',{'class':\"position-head-wrap-position-name\"}).get_text()\n",
    "        salary = bsobj.find('span',{'class':\"salary\"}).get_text()\n",
    "        other_info = bsobj.find('dd',{'class':\"job_request\"}).find('h3').get_text().split('/')\n",
    "        location = other_info[0].strip()\n",
    "        experience = other_info[1].strip()\n",
    "        edu_bg = other_info[2].strip()\n",
    "        company_name =  bsobj.find('div',{'class':\"job_company_content\"}).get_text()\n",
    "        industry = bsobj.findAll('h4',{'class':\"c_feature_name\"})[0].get_text()\n",
    "        company_scale = bsobj.findAll('h4',{'class':\"c_feature_name\"})[2].get_text()\n",
    "        position_describtion = bsobj.find('div',{'class':\"job-detail\"}).get_text()\n",
    "        \n",
    "        position_describtion = position_describtion.split(\"\\n\")\n",
    "        responsibility = []\n",
    "        demand = []\n",
    "        check_code = 0\n",
    "        for index, text in enumerate(position_describtion):\n",
    "            if check_code == 0 and re.match(r\"1|-|•\", text):\n",
    "                check_code = 1\n",
    "            if check_code == 1:\n",
    "                if re.match(r\"\\d|-|•\", text):\n",
    "                    responsibility.append(text)\n",
    "                else:\n",
    "                    check_code = 2\n",
    "            if check_code == 2 and re.match(r\"\\d|-|•\", text):\n",
    "                demand.append(text)\n",
    "        #如果上述筛选方式，无法拆分工作描述，则将其放到responsibility中\n",
    "        if check_code == 0:\n",
    "            responsibility = position_describtion\n",
    "        return [position_name,salary,location,experience,edu_bg,company_name,industry,company_scale,responsibility,demand]\n",
    "        \n",
    "def get_positionids():\n",
    "    key_id = defaultdict(list)\n",
    "    crawler = Crawler_for_positionid()\n",
    "    crawler.sign_in()\n",
    "    time.sleep(10)\n",
    "    key = crawler.key_manager.get_key()\n",
    "    id_list = crawler.get_positionid(key,job_num=10000)\n",
    "    key_id[key] = id_list\n",
    "    crawler.key_manager.save_key(key)\n",
    "    while(crawler.key_manager.wait_craw_key):\n",
    "        try:\n",
    "            key = crawler.key_manager.get_key()\n",
    "            id_list = crawler.get_positionid(key,job_num=10000)\n",
    "            key_id[key]=id_list\n",
    "            crawler.key_manager.save_key(key)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            with open('./key_id.json','r') as f:\n",
    "                a = json.load(f)\n",
    "            for key,value in key_id.items():\n",
    "                a[key] = value\n",
    "            with open('./key_id.json','w') as f:\n",
    "                json.dump(a,f)\n",
    "            WebDriverWait(crawler.webdriver,60).until(EC.presence_of_element_located((By.CLASS_NAME,\"position_link\")))\n",
    "    return key_id\n",
    "def get_ip():\n",
    "    response = requests.get(\"http://proxy.httpdaili.com/apinew.asp?sl=3&noinfo=true&text=1&ddbh=1448402233942353385\")\n",
    "    return response.text.split()\n",
    "#@pysnooper.snoop(prefix=\"get_info: \")\n",
    "def get_info(ki_file,destination_file):\n",
    "    #提取待爬id\n",
    "    info_dict = defaultdict(dict)\n",
    "    ip = 'http://'+get_ip()[-1]\n",
    "    ip_index = -1\n",
    "    with open(ki_file,'r') as f:\n",
    "        key_id = json.load(f)\n",
    "    with open(destination_file,'r') as f:\n",
    "        info_dict = json.load(f)\n",
    "        data = DataFrame(info_dict).T\n",
    "        count_group = data.groupby(data['key'])\n",
    "    with open('./temp.txt',encoding='utf-8') as f:\n",
    "        pass_key = f.read().split()\n",
    "    #pass_key = ['股票交易','数据仓库','数据库系统','需求调研']\n",
    "    for key,value in count_group: \n",
    "        if key in pass_key:\n",
    "            continue\n",
    "        if len(key_id[key])>value.shape[0]:\n",
    "            ids = list(set(key_id[key]) - set(value['id'].values))\n",
    "            print(len(ids))\n",
    "            for id_ in ids:\n",
    "                try:\n",
    "                    if id_ in info_dict.keys():\n",
    "                        continue\n",
    "                        print('id_已经出现过了')\n",
    "                    html = Crawler_for_positionid.html_from_positionid(ip,id_)\n",
    "                    info = Crawler_for_positionid.get_html_info(html)\n",
    "                    info_dict[id_]=\\\n",
    "                    {\n",
    "                        'id':id_,\n",
    "                        'key':key,\n",
    "                        'name':info[0],\n",
    "                        'salary':info[1],\n",
    "                        'experience':info[3],\n",
    "                        'edu_bg':info[4],\n",
    "                        'location':info[2],\n",
    "                        'company_name':info[5],\n",
    "                        'industry':info[6],\n",
    "                        'company_scale':info[7],\n",
    "                        'responsibility':info[8],\n",
    "                        'demand':info[9]\n",
    "                    }                 \n",
    "                    print(key,info_dict[id_]['id'])\n",
    "                    #time.sleep(0.5)\n",
    "                except (ProxyError,IndexError,AttributeError,MaxRetryError) as e:\n",
    "                    ip_index = (ip_index+1)%3\n",
    "                    ip = 'http://'+get_ip()[ip_index]\n",
    "                    print(e)\n",
    "                    with open(destination_file,'w') as f:\n",
    "                        json.dump(info_dict,f)\n",
    "                    continue            \n",
    "        with open('./temp.txt','a+',encoding='utf-8') as f:\n",
    "            f.write(key+' ')\n",
    "    new_keys = list(set(key_id.keys())-set(data['key'].unique()))\n",
    "    for key in new_keys:\n",
    "        for id_ in key_id[key]:\n",
    "            try:\n",
    "                if id_ in info_dict.keys():\n",
    "                    continue\n",
    "                    print('id_已经出现过了')\n",
    "                html = Crawler_for_positionid.html_from_positionid(ip,id_)\n",
    "                info = Crawler_for_positionid.get_html_info(html)\n",
    "                info_dict[id_]=\\\n",
    "                {\n",
    "                    'id':id_,\n",
    "                    'key':key,\n",
    "                    'name':info[0],\n",
    "                    'salary':info[1],\n",
    "                    'experience':info[3],\n",
    "                    'edu_bg':info[4],\n",
    "                    'location':info[2],\n",
    "                    'company_name':info[5],\n",
    "                    'industry':info[6],\n",
    "                    'company_scale':info[7],\n",
    "                    'responsibility':info[8],\n",
    "                    'demand':info[9]\n",
    "                }\n",
    "                print(key,info_dict[id_]['id'])\n",
    "                #time.sleep(0.5)              \n",
    "            except (ProxyError,IndexError,AttributeError,MaxRetryError) as e:\n",
    "                ip_index = (ip_index+1)%3\n",
    "                print(e)\n",
    "                ip = 'http://'+get_ip()[ip_index]\n",
    "                with open(destination_file,'w') as f:\n",
    "                    json.dump(info_dict,f)\n",
    "                continue \n",
    "    with open(destination_file,'w') as f:\n",
    "        json.dump(info_dict,f)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    #key_id = main()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "python 8333962\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "python 9808506\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "python 9782956\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "87\n",
      "list index out of range\n",
      "pytorch 7895018\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "pytorch 8296439\n",
      "list index out of range\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "list index out of range\n",
      "list index out of range\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "pytorch 9361240\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "pytorch 8708272\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "pytorch 9676278\n",
      "pytorch 8627905\n",
      "pytorch 9630275\n",
      "pytorch 9427992\n",
      "pytorch 6574872\n",
      "pytorch 9542642\n",
      "pytorch 8192366\n",
      "pytorch 8268857\n",
      "list index out of range\n",
      "pytorch 9374541\n",
      "pytorch 9787661\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "pytorch 4799197\n",
      "pytorch 9672453\n",
      "pytorch 9845373\n",
      "pytorch 9710109\n",
      "249\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "spark 9374752\n",
      "spark 9586383\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "spark 8270871\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/9782129.html (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F2E8595280>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。')))\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "spark 9332178\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "spark 5595840\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "spark 9790330\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "spark 9666027\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "spark 9119515\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "spark 8613567\n",
      "spark 9137649\n",
      "spark 9822807\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "spark 9824247\n",
      "spark 6633033\n",
      "spark 8150541\n",
      "spark 9555070\n",
      "spark 9677197\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "spark 9107272\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "spark 9041692\n",
      "spark 8815992\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "spark 9485162\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "59\n",
      "sql 6820086\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "sql 9790876\n",
      "sql 9611438\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "sql 7948438\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "sql 9060116\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "sql 9740262\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "sql 9566600\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "sql 9844742\n",
      "sql 9366230\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "sql 9747652\n",
      "sql 9696811\n",
      "175\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "临床试验统计 9841428\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "list index out of range\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "347\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "互联网数据 9281240\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/9608233.html (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F2E53EA250>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。')))\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/9802355.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001F2E0B6C940>, 'Connection to 121.61.163.31 timed out. (connect timeout=5)'))\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "128\n",
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/9683503.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001F2E53F5F40>, 'Connection to 121.61.163.31 timed out. (connect timeout=5)'))\n",
      "互联网运维 9732488\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/9665773.html (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F2E53F5520>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。')))\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/9821118.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001F2E59F07F0>, 'Connection to 121.61.163.31 timed out. (connect timeout=5)'))\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/9823715.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001F2EA6BC640>, 'Connection to 121.61.163.31 timed out. (connect timeout=5)'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/6634617.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001F2E3BC92B0>, 'Connection to 114.99.252.58 timed out. (connect timeout=5)'))\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/9777189.html (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F2E3BC94C0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。')))\n",
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/9250382.html (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F2EA6AA3A0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。')))\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/9784729.html (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F2EA6BC370>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。')))\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/9456922.html (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001F2E3BC9F70>, 'Connection to 121.61.163.31 timed out. (connect timeout=5)'))\n",
      "互联网运维 9414357\n",
      "互联网运维 7754751\n",
      "互联网运维 7007092\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/9002051.html (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F2E3A0AD30>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。')))\n",
      "互联网运维 9102502\n",
      "互联网运维 8996149\n",
      "互联网运维 9380200\n",
      "互联网运维 8359289\n",
      "互联网运维 8248694\n",
      "互联网运维 9719087\n",
      "互联网运维 8838914\n",
      "互联网运维 9068708\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/9627729.html (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F2E59E43A0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。')))\n",
      "互联网运维 9819841\n",
      "互联网运维 9778376\n",
      "互联网运维 9037925\n",
      "互联网运维 9264179\n",
      "互联网运维 9058407\n",
      "互联网运维 7465507\n",
      "互联网运维 9330518\n",
      "互联网运维 9247790\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "互联网运维 6967937\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "互联网运维 9734235\n",
      "互联网运维 9264953\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "互联网运维 9793428\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "互联网运维 8772287\n",
      "互联网运维 7160484\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "互联网运维 9780315\n",
      "互联网运维 8832709\n",
      "互联网运维 9135239\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "互联网运维 8218210\n",
      "互联网运维 6963795\n",
      "163\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "互联网运营 9844846\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "互联网运营 9540604\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "HTTPSConnectionPool(host='www.lagou.com', port=443): Max retries exceeded with url: /wn/jobs/9799749.html (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001F2E31A7F70>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。')))\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "互联网运营 9469090\n",
      "互联网运营 9699112\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "互联网运营 7351443\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "互联网运营 9624733\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "互联网运营 9842209\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "互联网运营 8880096\n",
      "互联网运营 9490100\n",
      "1\n",
      "319\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "交易模型 8869976\n",
      "交易模型 9724722\n",
      "交易模型 8643236\n",
      "交易模型 9843650\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "交易模型 8678025\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "交易模型 9607919\n",
      "交易模型 9214833\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "交易模型 8925506\n",
      "交易模型 8805401\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "交易模型 9204107\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "交易模型 9641407\n",
      "交易模型 9125663\n",
      "交易模型 9534054\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "交易模型 8537600\n",
      "交易模型 9482147\n",
      "交易模型 9518918\n",
      "交易模型 9027471\n",
      "交易模型 8264703\n",
      "交易模型 9758557\n",
      "交易模型 9663734\n",
      "交易模型 9726204\n",
      "交易模型 9663703\n",
      "交易模型 9482177\n",
      "交易模型 9622793\n",
      "交易模型 9111244\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "交易模型 8678041\n",
      "交易模型 7198955\n",
      "交易模型 8574145\n",
      "交易模型 9637034\n",
      "list index out of range\n",
      "交易模型 7149801\n",
      "交易模型 9622774\n",
      "交易模型 9640244\n",
      "交易模型 8978924\n",
      "386\n",
      "产品优化 6252469\n",
      "产品优化 6069229\n",
      "产品优化 6268531\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "产品优化 6079850\n",
      "产品优化 6086625\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "产品优化 6041726\n",
      "产品优化 6159046\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "产品优化 6197673\n",
      "产品优化 6265872\n",
      "产品优化 6061742\n",
      "产品优化 6074510\n",
      "产品优化 5834922\n",
      "产品优化 6181619\n",
      "产品优化 6214404\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "产品优化 5869378\n",
      "'NoneType' object has no attribute 'get_text'\n",
      "'NoneType' object has no attribute 'get_text'\n"
     ]
    },
    {
     "ename": "LocationParseError",
     "evalue": "Failed to parse: http://23:21:00",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLocationParseError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-5184a2761e3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'key_id.json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'data.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-987ebe942053>\u001b[0m in \u001b[0;36mget_info\u001b[1;34m(ki_file, destination_file)\u001b[0m\n\u001b[0;32m    125\u001b[0m                         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'id_已经出现过了'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m                     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrawler_for_positionid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml_from_positionid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mip\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mid_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m                     \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrawler_for_positionid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_html_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                     \u001b[0minfo_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mid_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-987ebe942053>\u001b[0m in \u001b[0;36mhtml_from_positionid\u001b[1;34m(self, ip, positionid)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.lagou.com/wn/jobs/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mpositionid\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".html\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'User-agent'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.81 Safari/537.36 SE 2.X MetaSr 1.0\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mhttp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProxyManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mip\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;31m#http = urllib3. PoolManager()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhttp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GET'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\studySoft\\anconda\\lib\\site-packages\\urllib3\\poolmanager.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, proxy_url, num_pools, headers, proxy_headers, **connection_pool_kw)\u001b[0m\n\u001b[0;32m    421\u001b[0m                 \u001b[0mproxy_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m             )\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[0mproxy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mproxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mport_by_scheme\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\studySoft\\anconda\\lib\\site-packages\\urllib3\\util\\url.py\u001b[0m in \u001b[0;36mparse_url\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLocationParseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m     \u001b[1;31m# For the sake of backwards compatibility we put empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\studySoft\\anconda\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mLocationParseError\u001b[0m: Failed to parse: http://23:21:00"
     ]
    }
   ],
   "source": [
    "\n",
    "get_info('key_id.json','data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1207\n"
     ]
    }
   ],
   "source": [
    "destination_file='data.json'\n",
    "with open(destination_file,'r') as f:\n",
    "    info_dict = json.load(f)\n",
    "    data = DataFrame(info_dict).T\n",
    "\n",
    "\n",
    "print(len(info_dict.keys()))\n",
    "print(data.loc['6267662',:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['股票交易', '统计模型', '产品优化', '运营管理', '金融风险', '需求分析', '风险模型', '营销策略', '信贷风险', '风控模型', '数据治理', '产品运营', '报表审计', '欺诈风险', '金融保险', '金融市场', '财务审计', '基金大数据', '数据库', '算法开发', '数据挖掘', '风险分析', '基金产品', '测试开发', '市场运营', '财务欺诈', '电商运营', '营销管理', '产品推广', '数据存储', '软件开发', 'excel', '量化研究', 'linux', 'hadoop', '语音数据', '用户推广', '数据反爬', '产品测试', '统计分析', '互联网运维', '指标体系', '数据库系统', '风险管理', '市场研究', '信贷模型', '基金报表', '模型开发', '财务指标', '金融业务', 'spark', '保险', '量化分析', '电商指标', '财务报表', '评分模型', '统计建模', '供应链大数据', 'sql', '人工智能', '统计研究', '营销分析', '供应链优化', '增长分析', '深度学习', '产品营销', '量化模型', '电商系统', '自然语言处理', '交易模型', '股票指标', '数据采集', '临床试验统计', '风险预测', '算法研究', '计算机视觉', 'pytorch', '电商交易', '大数据建模', '自动驾驶', '算法分析', '财务管理', '量化基金', '统计', '模型验证', '股票报表', 'hive', '电商大数据', '信贷欺诈', '算法测试', '信贷审计', '基金指标', '建模分析', '预测算法', '保险产品', '信贷大数据', '测试框架', '数据整理', '用户需求', '数据科学', '数据产品', '大数据系统', 'python', '编程开发', '量化交易', '互联网数据', '电商营销', '风险识别', '互联网运营', '股票预测', '精算师', '市场调研', '需求调研', '股票量化', '电商产品', '金融', 'java', '语音识别', '医学大数据', '量化指标', '产品调研', '量化算法', '机器学习', '编程', '算法设计', '金融模型', '市场分析', 'js', '市场量化', '市场营销', '量化策略', '数据结构', '系统开发', '大数据开发', '营销模型', '经济管理', '市场风控', '电商报表', '财务风险', '数据库开发', '产品指标', '数学', '大数据框架', '数据分析', '数据仓库', '交易市场', '审计精算', '软件系统', '基金管理', '计算机系统', '风控大数据', '医学数据', '社会经济', '电商经济', 'ab测试', '用户画像', '大数据', '数据系统', '供应链治理', '数字化管理', '股票风险'])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('key_id.json','r') as f:\n",
    "    key_id = json.load(f)\n",
    "key_id.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./temp.txt','a+',encoding='utf-8') as f:\n",
    "    for key in a.keys():\n",
    "        f.write(key+' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ip = '124.88.67.81:80'\n",
    "# ip='http://171.83.189.184:13456'\n",
    "# html = Crawler_for_positionid.html_from_positionid(ip,'9399072')\n",
    "# print(html.decode(\"UTF-8\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['反欺诈风控系统开发工程师 ',\n",
       " '20k-40k·14薪',\n",
       " '上海',\n",
       " '经验3-5年',\n",
       " '本科及以上',\n",
       " '阅文集团',\n",
       " '内容社区',\n",
       " '500-2000人',\n",
       " ['工作职责:参与风控系统的开发，负责反欺诈等新技术的研究和应用，结合风控业务具体问题，利用团伙识别、异常检测、聚类分析等构建用户行为反欺诈模风控型；负责风控策略的数据分析，为风控服务提供数据支撑；参与风控技术调研及核心模块的研发；制定、编写相关技术文档。任职资格:熟悉常用的机器学习算法，有数据挖掘项目经验；1年以上风控相关工作经验；熟练掌握Java、Python等编程语言；较强的逻辑思维能力，善于解决和分析有挑战的问题；有实际的风控系统开发经验，有反作弊、反欺诈开发经验；有较强责任感和荣誉感，对自己工作产出负责，既能发挥创造力，又不偏离公司及项目目标；具有良好的理解领悟能力、沟通能力，能够与团队形成良好配合，乐于承担工作压力。'],\n",
       " []]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Crawler_for_positionid.get_html_info(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.lagou.com/wn/jobs/9746367.html\"\n",
    "# #proxy = urllib3.ProxyManager('http://182.34.21.162:13456')\n",
    "# #proxy = urllib3.PoolManager()\n",
    "# r1 = proxy.request('GET',\"https://www.baidu.com\")\n",
    "# #r1 = proxy.request('GET',\"http://httpbin.org/ip\")\n",
    "# r1.data.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# proxies = { \"http\": \"http://localhost:9999\"}\n",
    "# response = requests.get(\"http://httpbin.org/ip\",proxies=proxies)\n",
    "# print(response.text)\n",
    "response = requests.get(\"http://proxy.httpdaili.com/apinew.asp?text=true&noinfo=true&sl=10&ddbh=2300692395120353385\")\n",
    "a = response.text.split()\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# proxy = {\n",
    "#     'http': '58.55.252.58:8080'\n",
    "# }\n",
    "\n",
    "# response = requests.get(\"http://httpbin.org/ip\",proxies=proxy)\n",
    "# print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "# import requests\n",
    "\n",
    "# url = \"http://myip.kkcha.com\"\n",
    "# #使用proxies构建一个字典的形式使用\n",
    "# proxies = {\"http\": \"122.6.226.55:8118\"}\n",
    "# response = requests.get(url=url, proxies=proxies)\n",
    "# print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
