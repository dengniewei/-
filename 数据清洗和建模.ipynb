{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "import numpy as np\n",
    "import os\n",
    "import jieba\n",
    "from collections import Counter,defaultdict\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"E:/code/jupyterNotebook/统计求职就业数据分析/test_data\"\n",
    "file_list = os.listdir(dir_path)\n",
    "data_ok = []\n",
    "add_words = ['大数据','机器学习','深度学习']\n",
    "for word in add_words:\n",
    "    jieba.add_word(word)\n",
    "#字符串清洗\n",
    "def clean_string(x):\n",
    "    pattern1 = r'[，。；、 . - ·]'\n",
    "    x = re.sub(pattern1,' ',x)\n",
    "    x = re.sub('[^\\u4e00-\\u9fa5 ^A-Z ^a-z]+', ' ', x)\n",
    "    return x\n",
    "#分词并剔除停用词\n",
    "def cut(x):\n",
    "    x = jieba.cut(x,cut_all=False)\n",
    "    x = \" \".join(x)\n",
    "    result = []\n",
    "    stop_words = []\n",
    "    with open('./stopwords.txt',encoding='utf-8') as f:\n",
    "        temp = f.readlines()\n",
    "        for word in temp:\n",
    "            stop_words.append(word.strip())        \n",
    "    for word in x.split(' '):\n",
    "        word = word.strip().lower()\n",
    "        if len(word)>1 and word not in stop_words :\n",
    "            result.append(word)\n",
    "    return result\n",
    "#导入数据\n",
    "for file_path in file_list:\n",
    "    data = pd.read_csv(os.path.join(dir_path,file_path),encoding='gbk')\n",
    "    data_ok.append(data)\n",
    "data = pd.concat(data_ok)\n",
    "#data.drop('Unnamed: 11',inplace=True,axis=1)\n",
    "data = data.drop_duplicates(['name','salary','experience','edu_bg','company_name'])\n",
    "data = DataFrame(data.values,columns = data.columns)\n",
    "#data = data[data['adapt']==1]\n",
    "string_feature = data[['responsibility','demand']].apply(lambda x:x.sum(),axis=1)\n",
    "string_feature = string_feature.map(clean_string)\n",
    "data['string_feature'] = string_feature.map(cut)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 抽取tf-idf词并组合形成关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7446\n"
     ]
    }
   ],
   "source": [
    "##实现TF-IDF\n",
    "#统计所有词的IDF\n",
    "##实现所有词的词频统计\n",
    "def tf(words):\n",
    "    outputwords = dict(Counter(words))\n",
    "    return outputwords\n",
    "def idf(text_list):#输入多个文本的分词列表\n",
    "    words_count = defaultdict(int)\n",
    "    idf = defaultdict(float)\n",
    "    sample_num = len(text_list)\n",
    "    for words in text_list:\n",
    "        words = set(words)\n",
    "        for word in words:\n",
    "            words_count[word] +=1\n",
    "    #计算IDF\n",
    "    for word,count in words_count.items():\n",
    "        idf[word] = math.log(sample_num/count+1)\n",
    "    return idf\n",
    "def tf_idf(tf,idf):\n",
    "    tf = Series(tf)\n",
    "    idf = Series(idf)\n",
    "    tf_idf = tf*idf\n",
    "    return tf_idf\n",
    "words =np.hstack(data['string_feature'].values)\n",
    "outputwords = tf(words)#输入分词列表\n",
    "idf_ = idf(data['string_feature'])\n",
    "tf_idf_= tf_idf(outputwords,idf_)\n",
    "#print(tf_idf_.sort_values(ascending=False)[:20])\n",
    "print(len(idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count=defaultdict(int)\n",
    "string_feature_grouped_by_key = data[['key','string_feature',]].groupby('key')\n",
    "for name,group in string_feature_grouped_by_key:\n",
    "    count = tf(np.hstack(group['string_feature']))\n",
    "    for key,value in count.items():\n",
    "        words_count[key] += value/group.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idf_ = idf(data['string_feature'])\n",
    "tf_idf_= tf_idf(words_count,idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从tf-idf前500个词中抽出合适的词\n",
    "#进行外连接，并筛选出100个合适的组合词\n",
    "words = pd.read_table('拓展关键词.txt')\n",
    "words_merge = np.array([0,0])\n",
    "for first in words['数据'].values:\n",
    "    for second in words['数据'].values:\n",
    "        if first != second:\n",
    "            words_merge = np.vstack([words_merge,np.array([first,second])])\n",
    "words_merge = words_merge[1:]\n",
    "words_merge = DataFrame(words_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用似然比检验搭建模型\n",
    "$$p(x|y)=\\frac {p(xy)} {p(y)}=\\frac {\\prod_{i=0}^n p(x_i y)} {p(y)}$$\n",
    "使用比例法定义概率\n",
    "$$p(x_i|y_0) = \\frac{x_i \\cap y_0} {n_0}$$\n",
    "$$p(x_i|y_1) = \\frac{x_i \\cap y_1} {n_1}$$\n",
    "其中 $x_i \\cap y_1$是参数y=1且词$x_i$出现的样本数,n_1是y=1的样本数.\n",
    "从而可以使用对数似然比进行分类，\n",
    "$$ if  {\\sum log((x_i \\cap y_1)+1)} - { \\sum log((x_i \\cap y_0)+1)} \\ge log \\frac {n_1}{n_0},y=1$$\n",
    "在实际使用中发现，将$log \\frac {n_1}{n_0}$调整为超参数$\\theta$，且修改上式，并使用交叉验证进行优化，分类效果更佳\n",
    "$$ if  \\frac {\\sum log((x_i \\cap y_1)+1)}{ \\sum log((x_i \\cap y_0)+1)} \\ge 1+\\theta,y=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成词矩阵\n",
    "outputwords_sorted = sorted(outputwords.items(), key= lambda x : x[1], reverse=True)\n",
    "word_index = defaultdict()\n",
    "for index,value in enumerate(outputwords_sorted):\n",
    "    word_index[value[0]] = index\n",
    "#稀疏矩阵\n",
    "words_matrix = np.zeros((data.shape[0],len(word_index)))\n",
    "for index,words in enumerate(data.loc[:,'string_feature']):\n",
    "    for word in words:\n",
    "        if word in word_index.keys():\n",
    "            words_matrix[index,word_index[word]]=1\n",
    "#X是词矩阵,Y=1表示合适，Y=0表示不合适\n",
    "X_Y= np.hstack([words_matrix,data['adapt'].values.reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(659, 7447) (309, 7447)\n"
     ]
    }
   ],
   "source": [
    "X_Y1 = X_Y[X_Y[:,-1]==1]\n",
    "X_Y0 = X_Y[X_Y[:,-1]==0]\n",
    "X_1_sum = X_Y1[:,:-1].sum(axis=0)#xi and y=1的事件发生次数\n",
    "X_0_sum = X_Y0[:,:-1].sum(axis=0)#xi and y=0的事件发生次数\n",
    "print(X_Y1.shape,X_Y0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LikehoodModel:\n",
    "    def train(self,X_Y):\n",
    "        X_Y1 = X_Y[X_Y[:,-1]==1]\n",
    "        X_Y0 = X_Y[X_Y[:,-1]==0]\n",
    "        self.X_1_sum = X_Y1[:,:-1].sum(axis=0)#xi and y=1的事件发生次数\n",
    "        self.X_0_sum = X_Y0[:,:-1].sum(axis=0)#xi and y=0的事件发生次数\n",
    "        \n",
    "    def generate_X(self,word_count,words_ser):#word_count是词频统计结果,words是所有监督数据的词列表Series\n",
    "        word_count = sorted(word_count.items(), key= lambda x : x[1], reverse=True)\n",
    "        word_index = defaultdict()\n",
    "        for index,value in enumerate(word_count):\n",
    "            word_index[value[0]] = index\n",
    "        #稀疏矩阵\n",
    "        words_matrix = np.zeros((words_ser.shape[0],len(word_index)))\n",
    "        for index,words in enumerate(words_ser):\n",
    "            for word in words:\n",
    "                if word in word_index.keys():\n",
    "                    words_matrix[index,word_index[word]]=1\n",
    "        return words_matrix\n",
    "    def predict(self,X):\n",
    "        Y_hat = np.zeros((X.shape[0],1))\n",
    "        for i in range(X.shape[0]):\n",
    "            loglikehood_1 = (np.log((self.X_1_sum[X[i,:]==1]+1).astype('float'))).sum()\n",
    "            loglikehood_0 = (np.log((self.X_0_sum[X[i,:]==1]+1).astype('float'))).sum()\n",
    "            \n",
    "            if loglikehood_1/loglikehood_0>1.18:\n",
    "                Y_hat[i,0] = 1\n",
    "        return Y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-a0482e69ae73>:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if loglikehood_1/loglikehood_0>1.18:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720.0\n",
      "968\n",
      "accuracy score:0.91\n",
      "recall score: 0.98\n",
      "precision score: 0.90\n"
     ]
    }
   ],
   "source": [
    "model = LikehoodModel()\n",
    "model.train(X_Y)\n",
    "y_hat = model.predict(X_Y[:,:-1])\n",
    "contrast = np.hstack([y_hat,X_Y[:,-1].reshape(-1,1).astype('float')])\n",
    "print(y_hat.sum())\n",
    "print(len(y_hat))\n",
    "print('accuracy score:{0:0.2f}'.format((contrast[:,0]==contrast[:,1]).sum()/contrast.shape[0]))\n",
    "\n",
    "#计算\n",
    "from sklearn.metrics import recall_score,precision_score\n",
    "recall = recall_score(contrast[:,1].reshape(-1), contrast[:,0].reshape(-1))\n",
    "precision = precision_score(contrast[:,1].reshape(-1), contrast[:,0].reshape(-1))\n",
    "print('recall score: {0:0.2f}'.format(\n",
    "      recall))\n",
    "print('precision score: {0:0.2f}'.format(\n",
    "      precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-a0482e69ae73>:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if loglikehood_1/loglikehood_0>1.18:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:0.81,recall score: 0.94,precision score: 0.81\n"
     ]
    }
   ],
   "source": [
    "#交叉验证\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10,shuffle=True,random_state=20211208)\n",
    "scores = np.zeros([3,10])\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X_Y):\n",
    "    train_XY,test_XY = X_Y[train_index],X_Y[test_index]\n",
    "    model = LikehoodModel()\n",
    "    model.train(train_XY)\n",
    "    y_hat = model.predict(test_XY[:,:-1])\n",
    "    contrast = np.hstack([y_hat,test_XY[:,-1].reshape(-1,1).astype('float')])\n",
    "    scores[0,i] = ((contrast[:,0]==contrast[:,1]).sum()/contrast.shape[0])\n",
    "    scores[1,i] = recall_score(contrast[:,1].reshape(-1), contrast[:,0].reshape(-1))\n",
    "    scores[2,i] = precision_score(contrast[:,1].reshape(-1), contrast[:,0].reshape(-1))\n",
    "    i += 1\n",
    "mean_scores = scores.mean(axis=1).reshape(-1)\n",
    "print('accuracy score:{0:0.2f},recall score: {1:0.2f},precision score: {2:0.2f}'.format(mean_scores[0],mean_scores[1],mean_scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.75257732 0.84536082 0.8556701  0.84536082 0.75257732 0.82474227\n",
      "  0.73195876 0.78350515 0.875      0.80208333]\n",
      " [0.82539683 0.90140845 0.93055556 0.96875    0.93220339 0.94285714\n",
      "  0.95081967 0.98305085 0.97183099 1.        ]\n",
      " [0.8        0.88888889 0.88157895 0.82666667 0.73333333 0.83544304\n",
      "  0.71604938 0.74358974 0.87341772 0.78409091]]\n",
      "[0.80688359 0.94068729 0.80830586]\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-842c883f5bfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLikehoodModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'string_feature'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_0_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_1_sum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: predict() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "#导入待判断数据\n",
    "# destination_file='data.json'\n",
    "# with open(destination_file,'r') as f:\n",
    "#     info_dict = json.load(f)\n",
    "#     data_test = DataFrame(info_dict).T\n",
    "data_test = data_test.drop_duplicates(['id'])\n",
    "data_test = DataFrame(data_test.values,columns = data_test.columns)\n",
    "#data = data[data['adapt']==1]\n",
    "data_test = data_test#.iloc[:2000,:]\n",
    "string_feature = data_test[['responsibility','demand']].apply(lambda x:x.sum(),axis=1)\n",
    "string_feature = string_feature.map(str)\n",
    "string_feature = string_feature.map(clean_string)\n",
    "data_test['string_feature'] = string_feature.map(cut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-a0482e69ae73>:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if loglikehood_1/loglikehood_0>1.18:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20540.0\n",
      "25131\n"
     ]
    }
   ],
   "source": [
    "words =np.hstack(data_test['string_feature'].values)\n",
    "model = LikehoodModel()\n",
    "model.train(X_Y)\n",
    "X = model.generate_X(outputwords,data_test['string_feature'])\n",
    "y_hat = model.predict(X)\n",
    "\n",
    "print(y_hat.sum())\n",
    "print(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将结果存入csv\n",
    "data_test['adapt']=y_hat\n",
    "data_test.to_csv('work_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
